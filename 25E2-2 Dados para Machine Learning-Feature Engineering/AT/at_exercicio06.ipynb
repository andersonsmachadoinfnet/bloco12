{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ce525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importações necessárias\n",
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63a57ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para baixar e extrair o dataset\n",
    "def download_and_extract_imdb(data_dir=\"aclImdb\"):\n",
    "    url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "    filename = \"aclImdb_v1.tar.gz\"\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(\"Baixando o dataset...\")\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        print(\"Extraindo os arquivos...\")\n",
    "        with tarfile.open(filename, \"r:gz\") as tar:\n",
    "            tar.extractall()\n",
    "        os.remove(filename)\n",
    "        print(\"Download e extração concluídos.\")\n",
    "    else:\n",
    "        print(\"Dataset já disponível.\")\n",
    "\n",
    "# Função para carregar resenhas com rótulos\n",
    "def load_reviews(data_dir, label, limit=None):\n",
    "    texts, labels = [], []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    for i, filename in enumerate(os.listdir(data_dir)):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(data_dir, filename), encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "                tokens = word_tokenize(text)\n",
    "                words = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "                cleaned_text = \" \".join(words)\n",
    "                texts.append(cleaned_text)\n",
    "                labels.append(label)\n",
    "\n",
    "            if limit and i + 1 >= limit:\n",
    "                break\n",
    "\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0fb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_extract_imdb()\n",
    "\n",
    "#Carregando resenhas positivas e negativas...\n",
    "pos_texts, pos_labels = load_reviews(\"aclImdb/train/pos\", 1, limit=1000)\n",
    "neg_texts, neg_labels = load_reviews(\"aclImdb/train/neg\", 0, limit=1000)\n",
    "\n",
    "texts = pos_texts + neg_texts\n",
    "labels = pos_labels + neg_labels\n",
    "\n",
    "#6.A-Convertendo texto em vetores TF-IDF...\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(texts)\n",
    "y = labels\n",
    "\n",
    "#6.B-Classificação com Regressão Logística...\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Avaliação do modelo:\")\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"Acurácia: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
